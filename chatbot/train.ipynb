{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11922584,"sourceType":"datasetVersion","datasetId":7495746}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers datasets peft accelerate bitsandbytes","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-11T17:46:43.459548Z","iopub.execute_input":"2025-06-11T17:46:43.459925Z","iopub.status.idle":"2025-06-11T17:48:06.003211Z","shell.execute_reply.started":"2025-06-11T17:46:43.459897Z","shell.execute_reply":"2025-06-11T17:48:06.002442Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport os\nimport shutil\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    Trainer,\n    TrainingArguments,\n    DataCollatorForLanguageModeling,\n)\nfrom peft import get_peft_model, LoraConfig, TaskType\nfrom datasets import load_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T17:48:06.004673Z","iopub.execute_input":"2025-06-11T17:48:06.004975Z","iopub.status.idle":"2025-06-11T17:48:34.503711Z","shell.execute_reply.started":"2025-06-11T17:48:06.004947Z","shell.execute_reply":"2025-06-11T17:48:34.502833Z"}},"outputs":[{"name":"stderr","text":"2025-06-11 17:48:19.454761: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749664099.656363      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749664099.720501      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Konfigurasi Model dan Path\nmodel_name = \"Qwen/Qwen2-1.5B-Instruct\"\ndataset_path = \"/kaggle/input/sampah-dataset/combined_all_datasets.jsonl\"\noutput_dir_training = \"./qwen-1.5B-lora-p100\"\noutput_dir_deployment = \"./qwen_1.5B_chatbot_p100_final\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T17:48:34.504615Z","iopub.execute_input":"2025-06-11T17:48:34.505302Z","iopub.status.idle":"2025-06-11T17:48:34.509233Z","shell.execute_reply.started":"2025-06-11T17:48:34.505276Z","shell.execute_reply":"2025-06-11T17:48:34.508576Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"print(\"--- Langkah 1: Memuat Tokenizer dan Model ---\")\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    print(f\"Tokenizer.pad_token diatur ke: {tokenizer.pad_token}\")\n\n# Pengaturan model untuk P100, memastikan semua menggunakan float16.\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    load_in_4bit=True,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,             # Gunakan float16 untuk memuat bobot model\n    bnb_4bit_compute_dtype=torch.float16,  # Tipe data komputasi untuk 4-bit, WAJIB float16 untuk P100\n)\n\nif model.config.pad_token_id is None or model.config.pad_token_id != tokenizer.pad_token_id:\n    model.config.pad_token_id = tokenizer.pad_token_id\n    print(f\"Model.config.pad_token_id diatur ke: {model.config.pad_token_id}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T17:48:34.511423Z","iopub.execute_input":"2025-06-11T17:48:34.511721Z","iopub.status.idle":"2025-06-11T17:48:56.769988Z","shell.execute_reply.started":"2025-06-11T17:48:34.511695Z","shell.execute_reply":"2025-06-11T17:48:56.769112Z"}},"outputs":[{"name":"stdout","text":"--- Langkah 1: Memuat Tokenizer dan Model ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a3dfde4463b4cf9bbd47b093a31c151"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c65a3d1746504f5a926814dc29e3f748"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d76d6c04bbf45fd952dc91da82c9414"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a990bd9f14248ad9055f28864423246"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"815b668abb46471a8350744f44471f52"}},"metadata":{}},{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b3494cb44a14ce4a23a69ea2b0493b8"}},"metadata":{}},{"name":"stderr","text":"Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"604e476f84624dc5b4db13cd7711e4c6"}},"metadata":{}},{"name":"stdout","text":"Model.config.pad_token_id diatur ke: 151643\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"print(\"\\n--- Langkah 2: Konfigurasi LoRA ---\")\n# Konfigurasi LoRA tidak perlu diubah\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\n        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n        \"gate_proj\", \"up_proj\", \"down_proj\"\n    ],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM,\n)\nmodel = get_peft_model(model, lora_config)\nmodel.enable_input_require_grads()\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T17:48:56.771033Z","iopub.execute_input":"2025-06-11T17:48:56.771369Z","iopub.status.idle":"2025-06-11T17:48:57.285677Z","shell.execute_reply.started":"2025-06-11T17:48:56.771340Z","shell.execute_reply":"2025-06-11T17:48:57.284744Z"}},"outputs":[{"name":"stdout","text":"\n--- Langkah 2: Konfigurasi LoRA ---\ntrainable params: 18,464,768 || all params: 1,562,179,072 || trainable%: 1.1820\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"print(\"\\n--- Langkah 3: Memuat dan Memproses Dataset ---\")\ndataset = load_dataset(\"json\", data_files=dataset_path)\nMAX_LENGTH = 512\n\ndef create_sft_datapoint(example):\n    instruction = example['instruction']\n    output_text = example['output']\n    user_content = instruction\n    messages = [{\"role\": \"user\", \"content\": user_content}, {\"role\": \"assistant\", \"content\": output_text}]\n    full_text_str = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n    model_input = tokenizer(full_text_str, max_length=MAX_LENGTH, truncation=True, padding=\"max_length\", return_attention_mask=True)\n    input_ids = model_input['input_ids']\n    attention_mask = model_input['attention_mask']\n    labels = list(input_ids)\n    prompt_messages = [{\"role\": \"user\", \"content\": user_content}]\n    prompt_only_str = tokenizer.apply_chat_template(prompt_messages, tokenize=False, add_generation_prompt=True)\n    tokenized_prompt_ids = tokenizer(prompt_only_str, max_length=MAX_LENGTH, truncation=True, add_special_tokens=False)['input_ids']\n    prompt_length = len(tokenized_prompt_ids)\n    for i in range(prompt_length):\n        if i < len(labels):\n            labels[i] = -100\n        else:\n            break\n    return {\"input_ids\": model_input['input_ids'], \"attention_mask\": model_input['attention_mask'], \"labels\": labels}\n\ntokenized_dataset_train = dataset[\"train\"].map(create_sft_datapoint, remove_columns=list(dataset[\"train\"].features))\nprint(\"Dataset berhasil diproses.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T17:48:57.286588Z","iopub.execute_input":"2025-06-11T17:48:57.286881Z","iopub.status.idle":"2025-06-11T17:49:04.687182Z","shell.execute_reply.started":"2025-06-11T17:48:57.286861Z","shell.execute_reply":"2025-06-11T17:49:04.686201Z"}},"outputs":[{"name":"stdout","text":"\n--- Langkah 3: Memuat dan Memproses Dataset ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"067be69380c743c2aea14f150362b72e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2529 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06cbc9dfd9d44d34b96a071f22d440a2"}},"metadata":{}},{"name":"stdout","text":"Dataset berhasil diproses.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"print(\"\\n--- Langkah 4: Konfigurasi dan Menjalankan Training (Optimized for P100) ---\")\ntraining_args = TrainingArguments(\n    output_dir=output_dir_training,\n    # PENYESUAIAN UNTUK P100: Naikkan batch size untuk utilisasi VRAM 16GB yang lebih baik\n    per_device_train_batch_size=2,\n    # PENYESUAIAN: Turunkan accumulation steps untuk menjaga effective batch size (8*2 = 16)\n    gradient_accumulation_steps=8,\n    gradient_checkpointing=True,\n    num_train_epochs=5,\n    learning_rate=4e-5,\n    fp16=True,                      # Aktifkan mixed-precision training, P100 mendukung ini\n    optim=\"paged_adamw_8bit\",\n    logging_steps=25,\n    save_strategy=\"steps\",\n    save_steps=50,\n    save_total_limit=2,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    report_to=\"none\",\n    disable_tqdm=False,\n)\n\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\ntrainer = Trainer(model=model, args=training_args, train_dataset=tokenized_dataset_train, data_collator=data_collator)\n\nprint(\"Memulai fine-tuning...\")\ntrainer.train()\nprint(\"Fine-tuning selesai.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T17:49:04.688103Z","iopub.execute_input":"2025-06-11T17:49:04.688440Z","iopub.status.idle":"2025-06-11T20:59:10.955695Z","shell.execute_reply.started":"2025-06-11T17:49:04.688405Z","shell.execute_reply":"2025-06-11T20:59:10.954821Z"}},"outputs":[{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"\n--- Langkah 4: Konfigurasi dan Menjalankan Training (Optimized for P100) ---\nMemulai fine-tuning...\n","output_type":"stream"},{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='790' max='790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [790/790 3:09:50, Epoch 4/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>2.214600</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.882700</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>1.655200</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.525600</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>1.482000</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.454300</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>1.368400</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.340200</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>1.333500</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.285500</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>1.273300</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.252700</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>1.217700</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.165600</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>1.146900</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.148200</td>\n    </tr>\n    <tr>\n      <td>425</td>\n      <td>1.129500</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>1.164400</td>\n    </tr>\n    <tr>\n      <td>475</td>\n      <td>1.121300</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.059800</td>\n    </tr>\n    <tr>\n      <td>525</td>\n      <td>1.062100</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>1.057100</td>\n    </tr>\n    <tr>\n      <td>575</td>\n      <td>1.034200</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.052500</td>\n    </tr>\n    <tr>\n      <td>625</td>\n      <td>1.069600</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>1.063400</td>\n    </tr>\n    <tr>\n      <td>675</td>\n      <td>1.017000</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>1.004000</td>\n    </tr>\n    <tr>\n      <td>725</td>\n      <td>1.012100</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>1.024900</td>\n    </tr>\n    <tr>\n      <td>775</td>\n      <td>1.013400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Fine-tuning selesai.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"print(f\"Menyimpan adapter LoRA model ke: {output_dir_deployment}...\")\n# 'model' di sini adalah PeftModel Anda yang sudah dilatih\nmodel.save_pretrained(output_dir_deployment)\nprint(\"Adapter LoRA berhasil disimpan.\")\n\nprint(f\"Menyimpan tokenizer ke: {output_dir_deployment}...\")\ntokenizer.save_pretrained(output_dir_deployment)\nprint(\"Tokenizer berhasil disimpan.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T20:59:10.956527Z","iopub.execute_input":"2025-06-11T20:59:10.956792Z","iopub.status.idle":"2025-06-11T20:59:11.470115Z","shell.execute_reply.started":"2025-06-11T20:59:10.956768Z","shell.execute_reply":"2025-06-11T20:59:11.469453Z"}},"outputs":[{"name":"stdout","text":"Menyimpan adapter LoRA model ke: ./qwen_1.5B_chatbot_p100_final...\nAdapter LoRA berhasil disimpan.\nMenyimpan tokenizer ke: ./qwen_1.5B_chatbot_p100_final...\nTokenizer berhasil disimpan.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!zip -r /kaggle/working/hasil_training.zip /kaggle/working/qwen_1.5B_chatbot_p100_final","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T20:59:11.470948Z","iopub.execute_input":"2025-06-11T20:59:11.471152Z","iopub.status.idle":"2025-06-11T20:59:17.135344Z","shell.execute_reply.started":"2025-06-11T20:59:11.471130Z","shell.execute_reply":"2025-06-11T20:59:17.134525Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/qwen_1.5B_chatbot_p100_final/ (stored 0%)\n  adding: kaggle/working/qwen_1.5B_chatbot_p100_final/vocab.json","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":" (deflated 61%)\n  adding: kaggle/working/qwen_1.5B_chatbot_p100_final/tokenizer.json (deflated 81%)\n  adding: kaggle/working/qwen_1.5B_chatbot_p100_final/merges.txt (deflated 57%)\n  adding: kaggle/working/qwen_1.5B_chatbot_p100_final/tokenizer_config.json (deflated 65%)\n  adding: kaggle/working/qwen_1.5B_chatbot_p100_final/README.md (deflated 66%)\n  adding: kaggle/working/qwen_1.5B_chatbot_p100_final/adapter_model.safetensors (deflated 7%)\n  adding: kaggle/working/qwen_1.5B_chatbot_p100_final/adapter_config.json (deflated 55%)\n  adding: kaggle/working/qwen_1.5B_chatbot_p100_final/added_tokens.json (deflated 36%)\n  adding: kaggle/working/qwen_1.5B_chatbot_p100_final/special_tokens_map.json (deflated 61%)\n","output_type":"stream"}],"execution_count":9}]}