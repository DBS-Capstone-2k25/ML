{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11922584,"sourceType":"datasetVersion","datasetId":7495746}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers datasets peft accelerate bitsandbytes","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-27T12:08:51.348086Z","iopub.execute_input":"2025-05-27T12:08:51.348492Z","iopub.status.idle":"2025-05-27T12:10:12.106174Z","shell.execute_reply.started":"2025-05-27T12:08:51.348458Z","shell.execute_reply":"2025-05-27T12:10:12.105436Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling\nfrom peft import get_peft_model, LoraConfig, TaskType\nfrom datasets import load_dataset\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T12:10:12.108031Z","iopub.execute_input":"2025-05-27T12:10:12.108275Z","iopub.status.idle":"2025-05-27T12:10:39.332931Z","shell.execute_reply.started":"2025-05-27T12:10:12.108250Z","shell.execute_reply":"2025-05-27T12:10:39.332139Z"}},"outputs":[{"name":"stderr","text":"2025-05-27 12:10:25.625949: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748347825.822557      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748347825.875862      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"model_name = \"Qwen/Qwen2-7B-Instruct\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T12:10:39.333662Z","iopub.execute_input":"2025-05-27T12:10:39.334146Z","iopub.status.idle":"2025-05-27T12:10:41.036683Z","shell.execute_reply.started":"2025-05-27T12:10:39.334127Z","shell.execute_reply":"2025-05-27T12:10:41.036077Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b5db862e6e04b81bf4ecfb613e9e7b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf1d85f2671348ca93f30eb82260aa06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4633779b06a4bb9aefd531039a62051"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"597f4e3d37fb4c72b90b0997465a61c0"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"if tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    print(f\"Tokenizer.pad_token diatur ke: {tokenizer.pad_token}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T12:10:41.037463Z","iopub.execute_input":"2025-05-27T12:10:41.037695Z","iopub.status.idle":"2025-05-27T12:10:41.041867Z","shell.execute_reply.started":"2025-05-27T12:10:41.037675Z","shell.execute_reply":"2025-05-27T12:10:41.041164Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    load_in_4bit=True,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    bnb_4bit_compute_dtype=torch.float16 \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T12:10:41.043616Z","iopub.execute_input":"2025-05-27T12:10:41.043931Z","iopub.status.idle":"2025-05-27T12:13:59.338323Z","shell.execute_reply.started":"2025-05-27T12:10:41.043913Z","shell.execute_reply":"2025-05-27T12:13:59.337794Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb6515a4553f4c12b722fde343d7382b"}},"metadata":{}},{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/27.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"607f35d28e5f48898cea2ae54ecb029b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4794e1a319bc4a078dd147ddc5608658"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94c59cf33eb94f36a99a3533e4f15409"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ce5d8b6a64044bebeceb523dd447c01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d68bc4c67e940648c8f472d94f5f834"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36c1e78ca6594059b4bf70425340350a"}},"metadata":{}},{"name":"stderr","text":"Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d091c3f54ea4651955e42fab3cefb10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d59bdf6725f47b7ab50a62a31d742bd"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"if model.config.pad_token_id is None or model.config.pad_token_id != tokenizer.pad_token_id:\n    model.config.pad_token_id = tokenizer.pad_token_id\n    print(f\"Model.config.pad_token_id diatur ke: {model.config.pad_token_id}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T12:13:59.339279Z","iopub.execute_input":"2025-05-27T12:13:59.339563Z","iopub.status.idle":"2025-05-27T12:13:59.343921Z","shell.execute_reply.started":"2025-05-27T12:13:59.339538Z","shell.execute_reply":"2025-05-27T12:13:59.343201Z"}},"outputs":[{"name":"stdout","text":"Model.config.pad_token_id diatur ke: 151643\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=32, # Coba tingkatkan r jika memungkinkan\n    lora_alpha=64, # Biasanya 2x dari r\n    target_modules=[\n        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n        \"gate_proj\", \"up_proj\", \"down_proj\"\n    ],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM\n)\nmodel = get_peft_model(model, lora_config)\nmodel.enable_input_require_grads()\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T12:13:59.344760Z","iopub.execute_input":"2025-05-27T12:13:59.344990Z","iopub.status.idle":"2025-05-27T12:14:00.432438Z","shell.execute_reply.started":"2025-05-27T12:13:59.344975Z","shell.execute_reply":"2025-05-27T12:14:00.431758Z"}},"outputs":[{"name":"stdout","text":"trainable params: 80,740,352 || all params: 7,696,356,864 || trainable%: 1.0491\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"dataset = load_dataset(\"json\", data_files=\"/kaggle/input/sampah-dataset/combined_all_datasets.jsonl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T12:14:00.433667Z","iopub.execute_input":"2025-05-27T12:14:00.433885Z","iopub.status.idle":"2025-05-27T12:14:00.674409Z","shell.execute_reply.started":"2025-05-27T12:14:00.433867Z","shell.execute_reply":"2025-05-27T12:14:00.673854Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8b10eac5e7749bba177367e2f718ace"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"MAX_LENGTH = 1024","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T12:14:00.675085Z","iopub.execute_input":"2025-05-27T12:14:00.675303Z","iopub.status.idle":"2025-05-27T12:14:00.678980Z","shell.execute_reply.started":"2025-05-27T12:14:00.675279Z","shell.execute_reply":"2025-05-27T12:14:00.678298Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def create_sft_datapoint(example):\n    instruction = example['instruction']\n    output_text = example['output']\n    user_content = instruction # Karena example['input'] kosong\n\n    messages = [\n        {\"role\": \"user\", \"content\": user_content},\n        {\"role\": \"assistant\", \"content\": output_text}\n    ]\n    \n    full_text_str = tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=False\n    )\n    model_input = tokenizer(\n        full_text_str,\n        max_length=MAX_LENGTH,\n        truncation=True,\n        padding=False,\n        return_attention_mask=True\n    )\n    input_ids = model_input['input_ids']\n    attention_mask = model_input['attention_mask']\n    labels = list(input_ids)\n\n    prompt_messages = [{\"role\": \"user\", \"content\": user_content}]\n    prompt_only_str = tokenizer.apply_chat_template(\n        prompt_messages, tokenize=False, add_generation_prompt=True\n    )\n    \n    tokenized_prompt_ids = tokenizer(\n        prompt_only_str, max_length=MAX_LENGTH, truncation=True, add_special_tokens=False\n    )['input_ids']\n    prompt_length = len(tokenized_prompt_ids)\n\n    for i in range(prompt_length):\n        if i < len(labels):\n            labels[i] = -100\n        else:\n            break\n            \n    if output_text and all(label == -100 for label in labels):\n        print(f\"Peringatan: Semua label di-mask untuk contoh: {instruction[:50]}... Prompt length: {prompt_length}, Input IDs length: {len(input_ids)}\")\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"labels\": labels\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T12:14:00.679830Z","iopub.execute_input":"2025-05-27T12:14:00.680063Z","iopub.status.idle":"2025-05-27T12:14:00.696421Z","shell.execute_reply.started":"2025-05-27T12:14:00.680048Z","shell.execute_reply":"2025-05-27T12:14:00.695766Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"tokenized_dataset_train = dataset[\"train\"].map(\n    create_sft_datapoint,\n    remove_columns=list(dataset[\"train\"].features) # Hapus kolom asli\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T12:14:00.697060Z","iopub.execute_input":"2025-05-27T12:14:00.697245Z","iopub.status.idle":"2025-05-27T12:14:03.191938Z","shell.execute_reply.started":"2025-05-27T12:14:00.697231Z","shell.execute_reply":"2025-05-27T12:14:03.190865Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2529 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c046b2b7368d4abf89cc7208995ad165"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./qwen-lora-sampah-t4x2\",\n    per_device_train_batch_size=1,  # ❗ Krusial untuk T4 7B. Mungkin bisa 2 jika MAX_LENGTH < 512.\n    gradient_accumulation_steps=16, # Sesuaikan untuk mencapai batch size efektif (misal, 1*2*16 = 32)\n    gradient_checkpointing=True,    # ❗ Sangat penting untuk hemat memori, walau training lebih lambat ~20-30%\n    num_train_epochs=3,             # 1-3 epoch biasanya cukup untuk LoRA\n    learning_rate=4e-5,             # ❗ Coba antara 2e-5 hingga 5e-5. 2e-4 terlalu tinggi.\n    fp16=True,                      # Wajib untuk T4\n    optim=\"paged_adamw_8bit\",       # ❗ Hemat memori untuk optimizer dengan 4-bit model\n    logging_steps=25,               # Sesuaikan berdasarkan total steps (misal, 10-50)\n    save_strategy=\"steps\",\n    save_steps=50,                  # Simpan checkpoint secara berkala\n    save_total_limit=2,             # Batasi jumlah checkpoint\n    lr_scheduler_type=\"cosine\",     # Pilihan scheduler yang baik\n    warmup_ratio=0.03,              # Sedikit warmup untuk stabilisasi training\n    report_to=\"none\",               # Set ke \"wandb\" atau \"tensorboard\" jika Anda menggunakannya\n    disable_tqdm=False,\n    # evaluation_strategy=\"steps\",    # Jika Anda punya eval dataset\n    # eval_steps=50,                  # Sesuaikan dengan save_steps\n    # load_best_model_at_end=True,    # Jika menggunakan evaluasi\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T12:14:03.192805Z","iopub.execute_input":"2025-05-27T12:14:03.193160Z","iopub.status.idle":"2025-05-27T12:14:03.241477Z","shell.execute_reply.started":"2025-05-27T12:14:03.193135Z","shell.execute_reply":"2025-05-27T12:14:03.240666Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T12:14:03.242367Z","iopub.execute_input":"2025-05-27T12:14:03.242559Z","iopub.status.idle":"2025-05-27T12:14:03.245967Z","shell.execute_reply.started":"2025-05-27T12:14:03.242545Z","shell.execute_reply":"2025-05-27T12:14:03.245460Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset_train, # Gunakan dataset yang sudah diproses\n    # eval_dataset=tokenized_dataset_eval, # Jika ada\n    data_collator=data_collator\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T12:14:03.248157Z","iopub.execute_input":"2025-05-27T12:14:03.248696Z","iopub.status.idle":"2025-05-27T12:14:03.281606Z","shell.execute_reply.started":"2025-05-27T12:14:03.248678Z","shell.execute_reply":"2025-05-27T12:14:03.281095Z"}},"outputs":[{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"print(\"Memulai fine-tuning...\")\ntrainer.train()\nprint(\"Fine-tuning selesai.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T12:14:03.282170Z","iopub.execute_input":"2025-05-27T12:14:03.282415Z","iopub.status.idle":"2025-05-27T15:18:08.905416Z","shell.execute_reply.started":"2025-05-27T12:14:03.282399Z","shell.execute_reply":"2025-05-27T15:18:08.904751Z"}},"outputs":[{"name":"stdout","text":"Memulai fine-tuning...\n","output_type":"stream"},{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='474' max='474' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [474/474 3:03:40, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>1.868100</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.339400</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>1.199300</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.106900</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>1.072500</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.042300</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>0.947100</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.878500</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>0.884500</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.848200</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>0.851000</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.822800</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>0.789500</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.711900</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>0.698100</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.696900</td>\n    </tr>\n    <tr>\n      <td>425</td>\n      <td>0.692000</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.709600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Fine-tuning selesai.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"output_direktori_model = \"./hasil_finetuning_qwen_chatbot\" # Anda bisa ganti nama direktorinya\n\nprint(f\"Menyimpan adapter LoRA model ke: {output_direktori_model}...\")\nmodel.save_pretrained(output_direktori_model)\nprint(\"Adapter LoRA berhasil disimpan.\")\n\nprint(f\"Menyimpan tokenizer ke: {output_direktori_model}...\")\ntokenizer.save_pretrained(output_direktori_model)\nprint(\"Tokenizer berhasil disimpan.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:18:08.906157Z","iopub.execute_input":"2025-05-27T15:18:08.906431Z","iopub.status.idle":"2025-05-27T15:18:09.687074Z","shell.execute_reply.started":"2025-05-27T15:18:08.906403Z","shell.execute_reply":"2025-05-27T15:18:09.686214Z"}},"outputs":[{"name":"stdout","text":"Menyimpan adapter LoRA model ke: ./hasil_finetuning_qwen_chatbot...\nAdapter LoRA berhasil disimpan.\nMenyimpan tokenizer ke: ./hasil_finetuning_qwen_chatbot...\nTokenizer berhasil disimpan.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"model.eval()\nprint(\"Model diatur ke mode evaluasi. Siap untuk diuji.\")\n\n# --- Fungsi untuk Chat Interaktif ---\n# Riwayat percakapan disimpan dalam format yang diharapkan oleh tokenizer.apply_chat_template\nmessages_history = [] \n\nprint(\"\\n--- Chatbot Interaktif Dimulai (Ketik 'quit' atau 'exit' untuk keluar) ---\")\nwhile True:\n    user_input = input(\"Anda: \")\n    if user_input.lower() in ['quit', 'exit']:\n        break\n\n    current_turn_messages = list(messages_history) \n    current_turn_messages.append({\"role\": \"user\", \"content\": user_input})\n\n    try:\n        prompt_chat_template = tokenizer.apply_chat_template(\n            current_turn_messages,\n            tokenize=False,\n            add_generation_prompt=True \n        )\n    except Exception as e:\n        print(f\"Error saat menerapkan chat template: {e}\")\n        continue\n\n    # Pindahkan input ke device yang sama dengan model Anda\n    # Jika 'model' Anda menggunakan device_map=\"auto\" atau sudah di .to(device) sebelumnya,\n    # tokenizer().to(model.device) akan menempatkannya dengan benar.\n    # Jika Anda tidak yakin, Anda bisa cek model.device\n    device_to_use = model.device \n    inputs = tokenizer(prompt_chat_template, return_tensors=\"pt\", padding=True).to(device_to_use)\n\n    print(\"Chatbot sedang berpikir...\")\n    with torch.no_grad(): \n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=300,\n            do_sample=True,      \n            temperature=0.6,     \n            top_k=50,            \n            top_p=0.95,          \n            pad_token_id=tokenizer.pad_token_id,\n            eos_token_id=tokenizer.eos_token_id \n        )\n    \n    response_ids = outputs[0][inputs.input_ids.shape[-1]:]\n    response_text = tokenizer.decode(response_ids, skip_special_tokens=True).strip()\n    \n    print(f\"Chatbot: {response_text}\")\n\n    messages_history.append({\"role\": \"user\", \"content\": user_input})\n    messages_history.append({\"role\": \"assistant\", \"content\": response_text})\n\n    if len(messages_history) > 10: \n        messages_history = messages_history[-10:]\n\nprint(\"Sesi interaktif selesai.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:49:46.231830Z","iopub.execute_input":"2025-05-27T15:49:46.232132Z","iopub.status.idle":"2025-05-27T15:50:33.761168Z","shell.execute_reply.started":"2025-05-27T15:49:46.232109Z","shell.execute_reply":"2025-05-27T15:50:33.760378Z"}},"outputs":[{"name":"stdout","text":"Model diatur ke mode evaluasi. Siap untuk diuji.\n\n--- Chatbot Interaktif Dimulai (Ketik 'quit' atau 'exit' untuk keluar) ---\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Anda:  sebutkan jenis jenis sampah\n"},{"name":"stdout","text":"Chatbot sedang berpikir...\nChatbot: Sampah umum, sampah B3, sampah anorganik, sampah organik, sampah kertas, sampah plastik, sampah logam, sampah elektronik, sampah fakta (sisa makanan). Sampah bisa dibagi berdasarkan sifatnya, jenis bahan, dan cara pengelolaannya. Pengelolaan yang tepat untuk setiap jenis sangat penting.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Anda:  exit\n"},{"name":"stdout","text":"Sesi interaktif selesai.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"output_direktori_untuk_deployment = \"./qwen_chatbot_huggingface_space\" # Ganti nama direktori sesuai keinginan\n\nprint(f\"Menyimpan adapter LoRA model ke: {output_direktori_untuk_deployment}...\")\n# 'model' di sini adalah PeftModel Anda yang sudah dilatih\nmodel.save_pretrained(output_direktori_untuk_deployment)\nprint(\"Adapter LoRA berhasil disimpan.\")\n\nprint(f\"Menyimpan tokenizer ke: {output_direktori_untuk_deployment}...\")\ntokenizer.save_pretrained(output_direktori_untuk_deployment)\nprint(\"Tokenizer berhasil disimpan.\")\n\nprint(f\"Model dan tokenizer siap untuk deployment disimpan di: {output_direktori_untuk_deployment}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:53:35.502909Z","iopub.execute_input":"2025-05-27T15:53:35.503546Z","iopub.status.idle":"2025-05-27T15:53:36.437773Z","shell.execute_reply.started":"2025-05-27T15:53:35.503519Z","shell.execute_reply":"2025-05-27T15:53:36.436872Z"}},"outputs":[{"name":"stdout","text":"Menyimpan adapter LoRA model ke: ./qwen_chatbot_huggingface_space...\nAdapter LoRA berhasil disimpan.\nMenyimpan tokenizer ke: ./qwen_chatbot_huggingface_space...\nTokenizer berhasil disimpan.\nModel dan tokenizer siap untuk deployment disimpan di: ./qwen_chatbot_huggingface_space\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"!zip -r /kaggle/working/qwen_chatbot_huggingface_space.zip /kaggle/working/qwen_chatbot_huggingface_space","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:56:52.911121Z","iopub.execute_input":"2025-05-27T15:56:52.911877Z","iopub.status.idle":"2025-05-27T15:57:13.651917Z","shell.execute_reply.started":"2025-05-27T15:56:52.911847Z","shell.execute_reply":"2025-05-27T15:57:13.650741Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/qwen_chatbot_huggingface_space/ (stored 0%)\n  adding: kaggle/working/qwen_chatbot_huggingface_space/adapter_config.json (deflated 56%)\n  adding: kaggle/working/qwen_chatbot_huggingface_space/merges.txt","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":" (deflated 57%)\n  adding: kaggle/working/qwen_chatbot_huggingface_space/adapter_model.safetensors (deflated 8%)\n  adding: kaggle/working/qwen_chatbot_huggingface_space/added_tokens.json (deflated 36%)\n  adding: kaggle/working/qwen_chatbot_huggingface_space/tokenizer_config.json (deflated 65%)\n  adding: kaggle/working/qwen_chatbot_huggingface_space/tokenizer.json (deflated 81%)\n  adding: kaggle/working/qwen_chatbot_huggingface_space/README.md (deflated 66%)\n  adding: kaggle/working/qwen_chatbot_huggingface_space/vocab.json (deflated 61%)\n  adding: kaggle/working/qwen_chatbot_huggingface_space/special_tokens_map.json (deflated 61%)\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}